{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arunpothencode/MyPython/blob/Dev/Spark_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf7joo_yc6ea",
        "outputId": "77148b94-8b1c-49f4-ba17-2fbd7468af3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.functions import udf, explode, col, regexp_replace\n",
        "from pyspark.sql.types import IntegerType,StringType,ArrayType\n",
        "\n",
        "spark = SparkSession.builder.appName('Spark test').getOrCreate()\n",
        "\n",
        "data = [\n",
        "    (1, \"This is sentence one. This is sentence two! I'm Sentence 3? Hello world.\"),\n",
        "    (2, \"To be, or not to be, that is the question. Whether it is nobler in the mind to suffer. The slings and arrows of outrageous fortune.\"),\n",
        "    (3, \"It was a soft, reposeful summer landscape, as lovely as a dream, and as lonesome as Sunday. The air was full of the smell of flowers, and the buzzing of insects, and the twittering of birds, and there were no people, no wagons, there was no stir of life, nothing going on. The road was mainly a winding path with hoof-prints in it, and now and then a faint trace of wheels on either side in the grass—wheels that apparently had a tire as broad as one’s hand.\"),\n",
        "    (4, \"Abc aaa. Aaa abc def xyz.\"),\n",
        "    (5, \"abc,abc.abc!abc?\")\n",
        "  ]\n",
        "\n",
        "columns = [\"id\",\"text\"]\n",
        "df = spark.createDataFrame(data=data, schema = columns)\n",
        "\n",
        "#df.show(truncate=False)\n",
        "\n",
        "#   Task: write Spark code to return the words occuring more than once for each row\n",
        "#   Example:\n",
        "#   +---+---------------------------------------------------------------------+\n",
        "#   |id |word frequency                                                       |\n",
        "#   +---+---------------------------------------------------------------------+\n",
        "#   |1  |this:2, is:2, sentence:3                                             |\n",
        "#   |2  |to:3, be:2, is:2, the:3                                              |\n",
        "#   |3  |it:2, was:4, a:5, as:6, and:6, the:6, of:6, there:2, no:3, on:2, in:2|\n",
        "#   |4  |abc:2, aaa:2                                                         |\n",
        "#   |5  |abc:4                                                                |\n",
        "#   +---+---------------------------------------------------------------------+\n",
        "\n",
        "# def value_counts(textcol):\n",
        "#   countdict = {}\n",
        "#   for i in textcol.split(\" \"):\n",
        "#     if i in countdict.keys:\n",
        "#       countdict[i] += 1\n",
        "#     else:\n",
        "#       countdict[i] = 1\n",
        "#   finaldict = {}\n",
        "#   for k,v in countdict.items():\n",
        "#     if v>1:\n",
        "#       finaldict[k] = countdict[k]\n",
        "#   return finaldict\n",
        "\n",
        "# value_count_udf = udf(value_counts,StringType())\n",
        "# df = df.withColumn(\"word frequency\", value_count_udf(col(\"text\")))\n",
        "# df.show(truncate=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def value_counts(textcol):\n",
        "  countdict = {}\n",
        "  for i in textcol.split(\" \"):\n",
        "    word = str(i).lower()\n",
        "    if i in countdict.keys():\n",
        "      countdict[word] += 1\n",
        "    else:\n",
        "      countdict[word] = 1\n",
        "  finaldict = {}\n",
        "  for k,v in countdict.items():\n",
        "    if v>1:\n",
        "      finaldict[k] = countdict[k]\n",
        "  return finaldict"
      ],
      "metadata": {
        "id": "b_9sQu8rTQiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "value_counts(\"This is sentence one. This is sentence two! I'm Sentence 3? Hello world.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RI3TttdSA4i",
        "outputId": "d6774ed4-1d74-4b68-baba-457f9cbf2787"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'is': 2}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NR6O1fcDMp5V",
        "outputId": "46a7590f-4de9-4b15-e076-d07b14235cdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|id |text                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
            "+---+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|1  |This is sentence one. This is sentence two! I'm Sentence 3? Hello world.                                                                                                                                                                                                                                                                                                                                                                                                  |\n",
            "|2  |To be, or not to be, that is the question. Whether it is nobler in the mind to suffer. The slings and arrows of outrageous fortune.                                                                                                                                                                                                                                                                                                                                       |\n",
            "|3  |It was a soft, reposeful summer landscape, as lovely as a dream, and as lonesome as Sunday. The air was full of the smell of flowers, and the buzzing of insects, and the twittering of birds, and there were no people, no wagons, there was no stir of life, nothing going on. The road was mainly a winding path with hoof-prints in it, and now and then a faint trace of wheels on either side in the grass—wheels that apparently had a tire as broad as one’s hand.|\n",
            "|4  |Abc aaa. Aaa abc def xyz.                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
            "|5  |abc,abc.abc!abc?                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
            "+---+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def value_counts(textcol):\n",
        "  countdict = {}\n",
        "  for i in textcol.split(\" \"):\n",
        "    countdict[lower(i)] += 1\n",
        "  finaldict = {}\n",
        "  for k,v in countdict:\n",
        "    if v>1:\n",
        "      finaldict[k] = countdict[k]\n",
        "  return finaldict"
      ],
      "metadata": {
        "id": "tdI0-g7eOkXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "value_count_udf = udf(value_counts,StringType())"
      ],
      "metadata": {
        "id": "-TatZnd9P6-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = df.withColumn(\"word frequency\", value_count_udf(col(\"text\")))\n",
        "df2.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        },
        "id": "Jcg0vnyVNPOC",
        "outputId": "5ec504ea-cbd7-44fa-c9c0-3ae1c7eb40ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "PythonException",
          "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"<ipython-input-14-e09b7c9d5dac>\", line 4, in value_counts\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 174, in wrapped\n    return f(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 8524, in lower\n    return _invoke_function_over_columns(\"lower\", col)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 105, in _invoke_function_over_columns\n    return _invoke_function(name, *(_to_java_column(col) for col in cols))\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 105, in <genexpr>\n    return _invoke_function(name, *(_to_java_column(col) for col in cols))\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/column.py\", line 63, in _to_java_column\n    jcol = _create_column_from_name(col)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/column.py\", line 55, in _create_column_from_name\n    sc = get_active_spark_context()\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 248, in get_active_spark_context\n    raise RuntimeError(\"SparkContext or SparkSession should be created first.\")\nRuntimeError: SparkContext or SparkSession should be created first.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-861aed02706f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word frequency\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_count_udf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    945\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mBob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m         \"\"\"\n\u001b[0;32m--> 947\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m     def _show_string(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m_show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    976\u001b[0m                 )\n\u001b[1;32m    977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint_truncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"<ipython-input-14-e09b7c9d5dac>\", line 4, in value_counts\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 174, in wrapped\n    return f(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 8524, in lower\n    return _invoke_function_over_columns(\"lower\", col)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 105, in _invoke_function_over_columns\n    return _invoke_function(name, *(_to_java_column(col) for col in cols))\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 105, in <genexpr>\n    return _invoke_function(name, *(_to_java_column(col) for col in cols))\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/column.py\", line 63, in _to_java_column\n    jcol = _create_column_from_name(col)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/column.py\", line 55, in _create_column_from_name\n    sc = get_active_spark_context()\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 248, in get_active_spark_context\n    raise RuntimeError(\"SparkContext or SparkSession should be created first.\")\nRuntimeError: SparkContext or SparkSession should be created first.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B8lPtLJaOB0B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}